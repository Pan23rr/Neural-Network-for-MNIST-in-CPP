# ğŸ§  Neural Network From Scratch in C++

This project is a simple yet complete implementation of a Feedforward Neural Network written entirely in **C++** with no external ML libraries.

ğŸ› ï¸ **Goal:** Learn the inner workings of neural networks by building every component â€” forward propagation, backpropagation, weight updates, and evaluation â€” from the ground up.

---

## ğŸš€ Features

- Loads and processes the Fashion MNIST dataset
- Implements:
  - ReLu and Softmax activations
  - Categorical cross-entropy loss
  - Batch training
  - Gradient descent
  - Manual Backpropagation algorithm
- Custom training and testing loops
- Accuracy and loss tracking per batch
- Built-in benchmarking system

---

## ğŸ“Š Comparison with TensorFlow

| Feature                       | C++ Implementation     | TensorFlow        |
|-------------------------------|------------------------|-------------------|
| Library Dependency            | None                   | TensorFlow        |
| Training Speed                | Slower (manual)        | Optimized         |
| Control over internals        | Full                   | Abstracted        |
| GPU Support                   | âŒ                     | âœ…               |

### ğŸ“ˆ Accuracy Curve Comparison

**C++ Neural Network Accuracy over Batches:**

![C++ Accuracy](images/cpp_accuracy.png)

**TensorFlow Model Accuracy over 10 Epochs:**

![TensorFlow Accuracy](images/tf_accuracy.png)

---


## ğŸ“Ÿ Sample Output:

![Sample Output](images/result.png)

## âš™ï¸ How to Build and Run

```bash
# Clone the repository
git clone https://github.com/Pan23rr/Neural-Network-for-MNIST-in-CPP.git
cd Neural-Network-for-MNIST-in-CPP

# Compile (Linux example)
g++ train.cpp -o train.exe

# Run training
./train.exe



# ğŸ§  Neural Network From Scratch in C++

This project is a full implementation of a Feedforward Neural Network written entirely in **C++**, without using any machine learning libraries. It was built to understand the core mathematics and training logic behind neural networks â€” including forward propagation, backpropagation, activation functions, softmax, and weight updates.

---

## ğŸ¯ Goal

To learn and implement the internal workings of a neural network from scratch by:
- Loading and preprocessing MNIST-style data
- Manually coding every part: forward pass, gradients, backpropagation
- Training the model with mini-batch gradient descent
- Evaluating accuracy and tuning hyperparameters

---

## ğŸš€ Features

- Loads and processes the **Fashion MNIST** dataset (same format as original MNIST)
- Implements:
  - Fully connected dense layers
  - ReLU and Softmax activations
  - Categorical Cross-Entropy Loss
  - Mini-batch training
  - Accuracy evaluation
- Manual learning rate tuning
- Tracks best accuracy during training
- Lightweight (STL only), no external libraries

---

## ğŸ› ï¸ Getting Started

### 1. ğŸ§¾ Prerequisites

- C++ compiler (G++ recommended)
- CMake (optional for project organization)
- Fashion MNIST dataset (IDX format)

### 2. ğŸ“¦ Folder Structure for Dataset

Place the dataset in the following structure:

```bash
data/
â”œâ”€â”€ train-images.idx3-ubyte
â”œâ”€â”€ train-labels.idx1-ubyte
â”œâ”€â”€ t10k-images.idx3-ubyte
â””â”€â”€ t10k-labels.idx1-ubyte
```

You can download the dataset from [here](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/).

### 3. âš™ï¸ Compile and Run

```bash
g++ -O2 main.cpp -o neural_net
./neural_net

## ğŸ“š Architecture Overview

The neural network is composed of:

- **Input Layer**: 784 nodes (28Ã—28 image)
- **Hidden Layers**: Tunable (e.g., 128 or 64 neurons)
- **Output Layer**: 10 nodes (Softmax for 10 classes)

### ğŸ”§ Activation Functions
- **ReLU** in hidden layers
- **Softmax** in output layer

### ğŸ“‰ Loss Function
- **Categorical Cross-Entropy**

---

## ğŸ” Training Loop

1. **Forward Pass** â€“ Calculate activations layer by layer  
2. **Loss Calculation** â€“ Compute Softmax + Cross Entropy loss  
3. **Backpropagation** â€“ Derive and apply gradients  
4. **Weight Update** â€“ Update weights using SGD (with manually tuned learning rate)  
5. **Evaluation** â€“ Track and log maximum accuracy on the test set

---

## ğŸ§ª Hyperparameter Tuning

You can experiment with:
- **Learning rate** (`lr`)
- **Number of hidden layers**
- **Batch size**
- **Epoch count**

With the current setup, you can achieve around **85â€“90% accuracy** on the Fashion MNIST dataset. With further tuning or architectural improvements, the accuracy can go even higher.

---

## ğŸ”„ Saving Model (Optional)

Use the provided `saveModel` and `loadModel` functions (work in progress) to:
- ğŸ’¾ **Save** the best-performing weights and biases to a file
- ğŸ“‚ **Load** them later for evaluation or inference


## ğŸ“ˆ Performance vs Frameworks

| Feature            | This Project (C++) | TensorFlow Model |
|-------------------|--------------------|------------------|
| **Accuracy**       | ~85â€“90%            | ~95â€“97%          |
| **Flexibility**    | Low                | High             |
| **Educational Value** | High           | Medium           |
| **Reusability**    | Low                | Very High        |

> This project prioritizes **learning over performance**. Unlike frameworks, everything is manually coded to **demystify the internals**.

---

## ğŸ’¡ Notes & Reflections

This project started as a **personal challenge** â€” to build a neural network from scratch in just a week.

There were a lot of rough patches along the way â€” bugs in gradients, dimension mismatches, exploding values â€” all of which forced me to go back to the math, sketch things out in a notebook, and rebuild piece by piece.

After a lot of **messing around with learning rates** and tuning the batch size, I finally reached a working model that could consistently perform well on Fashion MNIST. This helped me truly understand how **backpropagation works**, and why frameworks do what they do under the hood.

---

## ğŸ“Œ Future Improvements

- âœ… Modularize the code into `Layer`, `Model`, `Trainer` classes  
- âœ… Add more activation functions and optimizers  
- âœ… Configurable architecture via CLI or JSON  
- âœ… Add visualizations or plots  
- âœ… Better documentation and comments  

---

## ğŸ™ Credits

- Fashion MNIST Dataset from [Zalando Research](https://github.com/zalandoresearch/fashion-mnist)  
- Inspired by educational efforts to deeply understand ML foundations

---

## ğŸ“¬ Contact

If you have any suggestions or improvements, feel free to **raise an issue** or **fork the project**!
